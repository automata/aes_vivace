\documentclass
  [ams,pdfout]% class options
        {aesbr}
\RequirePackage[utf8]{inputenc}
\RequirePackage[english,brazil]{babel}
\RequirePackage{textcomp}
\RequirePackage{url}
\begin{document}
\begin{TitlePage}
        \Title{Produção de áudio com Web Audio}
        \RunningTitle{Produção de áudio com Web Audi} % short title
        \RunningAuthors{Fulano et al.}
        % melhor colcar vc como primeiro autor
        % pois não estou afiliado a nenhuma universidade mais :P 
        \Author
                [fulano@email.org]% email
                {Fulano de Tal}% Author name (printed in title page)
                \Affil
                        [A]% chave/key (optional argument)
                        {%
                                Universidade,
                                Instituto,
                                Departamento\\
                                Cidade, Estado, Código Postal, País
                        }
        \Author
                [gcravista@gmail.com]
                {Guilherme Martins Lunhani}
                \Affilref[A]
                {
                	Licenciado em música pelo, 
                	Departamento de Música da UNICAMP, 
                	Ubatuba, SP, Brasil }
        \Abstract{%
        	Tecnologias Web têm se apresentado como um catalisador para o uso
        	criativo de mídias na Internet. Embora essas tecnologias tenham sido historicamente usadas em aplicações visuais, há recentemente uma preocupação em desenvolver interfaces para o processamento de áudio em navegadores Web. Web Audio API é uma interface que está sendo padronizada para o processamento de áudio em tempo real na Web. Através dela é possível desenvolver aplicações que vão desde sistemas de composição musical em tempo real até mixers distribuídos e colaborativos, sem a necessidade da instalação de plugins proprietários ou configurações complexas. Procurando  contribuir para o desenvolvimento desta tecnologi, introduzimos a Web Audio API através de exemplos simples, na forma de um tutorial e apresentamos aplicações que foram desenvolvidas utilizando essa tecnologia. }
\end{TitlePage}

\section*{}

Recentemente observamos um encorajamento no desenvolvimento de bibliotecas dedicadas ao processamento de áudio digital em navegadores da \emph{internet}(Web Audio API), o que viabilizou projetos como dataflow-webaudio\footnote{http://forresto.github.com/dataflow-webaudio/, acessado em fevereiro de 2013} e Mix.js\footnote{http://kevincennis.com/mix/, \emph{ibd.}}. No Brasil, o grupo LabMacambira.sf.net\footnote{Grupo de desenvolvedores de software livre, http://labmacambira.sourceforge.net/, ibd.} conta com experimentos como Vivace\footnote{https://github.com/automata/vivace}. Este último é uma proposta de \emph{live coding}\cite{nilson2007live,collins2003live}, no qual é realizada uma performance artística junto a um público, onde intérpretes editam e executam códigos de programação utilizados para processar materiais sonoros e visuais \emph{in live}. 

Processos de \emph{criação}, \emph{edição} e \emph{mixagem} são transformados com uso de \emph{aplicativos web}; neste artigo faremos uma breve descrição técnica da Web Audio API, das ferramentas desenvolvidas, e reflexões de seus usos em atividades de produção musical.

\section{CRIAÇÃO, EDIÇÃO E MIXAGEM: UMA REVISÃO DE CONCEITOS E FERRAMENTAS}

Os campos de produção musical e visual compartilham destes termos, onde cada um possui conceitos, ferramentas e técnicas próprias. Neste trabalho trataremos daqueles da produção musical, assim como explanado por \cite{izhaki2008mix}

\begin{figure}[htpb]
  \begin{center}
    \includegraphics[scale=.4]{mix_chain.png}
    \caption{Etapas do processo de produção musical (retirado de IZHAKI)}
    \label{figure:mix_chain}
  \end{center}
\end{figure}

Criação, edição e mixagem são etapas separadas; o primeiro pode adquirir facetas individuais,  enquanto editar e mixar podem se  apresentar como:

\begin{itemize}
    \item edição seletiva: processo de seleção dos materiais sonoros mais adequados, bem como o processo de combinar os materiais escolhidos em um \emph{take} mestre \cite{izhaki2008mix}.
    \item edição corretiva: processo onde se corrige partes defeituosas dos materiais escolhidos, que podem ter causas diversas: má performance do intérprete, captação inadequada, entre outros \cite{izhaki2008mix}.
    \item mixagem: Uma etapa do processo de produção musical no qual os materiais - amostrados ou sintetizados - são balanceados, tratados e combinados entre si em um formato multicanal, mais comumente em estereo de dois canais \cite{izhaki2008mix}.
\end{itemize}

Em grandes estúdios essas etapas se dividem entre o compositor, arranjador, produtor musical e engenheiros de som. De maneira bastante superficial, os dois últimos utilizam um computador com \emph{softwares}(como ProTools, Logic, Nuendo), que pode conter etodas as ferramentas de edição e mixagem.

Acreditamos que com ferramentas \emph{web} podemos agrupar as  três etapas em questão \footnote{Essas três atividades agrupadas é algo que experenciamos durante o \emph{live-coding}: o código-fonte é interpretado pelo computador e logo em seguida transformado em material sonoro por processamento de áudio digital (DSP)} e sociabilizar a produção musical. A criação deixa de ser individual, a edição deixa de ser apenas seletiva ou corretiva, a mixagem deixa de ser apenas uma maneira de balançear os materiais, uma vez que todos podem influenciar em qualquer parte do processo de produção.

\section{WEB AUDIO}

Historicamente as tecnologias Web estiverem focadas em aplicações gráficas. O design de interfaces com o usuário era até então prioridade. Recentemente, a necessidade pela manipulação de mídias fez surgir APIs implementadas diretamente em navegadores Web, que oferecem a possibilidade de manipular áudio e vídeo em tempo real, através de JavaScript. Com o desenvolvimento da versão 5 do padrão HTML, houve a criação de métodos e modelos para manipulação de imagens (através de procedimentos em JavaScript que permitem processar pixel a pixel a matriz de uma imagem, inserida em uma \emph{tag} $<canvas>$), vídeos (por procedimentos que manipulam \emph{tags} $<video>$) e áudio (\emph{tags} $<audio>$).

Porém, aplicações como a síntese e demais operações de processamento de áudio em tempo real não eram possíveis, mesmo usando tais \emph{tags}. A única forma de se processar áudio em tempo real era através do uso de plugins proprietários como \emph{Flash}. Recentemente, Dave Humphrey e colaboradores iniciaram o desenvolvimento de APIs que permitem acesso direto à interface de áudio a partir do browser. A primeira implementação de uma API deste tipo ficou conhecida como Audio Data API, implementada no navegador Mozilla Firefox. Com esse esforço e o reconhecimento da comunidade Web da necessidade de APIs para processamento de áudio em navegadores, o consórcio W3 criou um grupo de trabalho que terminou com o desenvolvimento de uma API padrão para processamento de audio. Essa API ficou conhecida como Web Audio. 

Web Audio é escrita em código nativo (C++ e Assembly) para garantir performance máxima no processamento de áudio em navegadores \emph{Web}\footnote{Atualmente, poucos navegadores tem suporte completo e utilizamos aquele distribuido pelos desenvolvedores \emph{WebKit} (i.e. \emph{Safari} e \emph{Chromium}. Uma implementação para o navegador Mozilla Firefox está em atual desenvolvimento.}. É baseada no paradigma de \emph{grafos de unidades de áudio}, onde se especifica uma coleção de nós e rotinas de conexão e desconexão entre eles (Figura \ref{figure:graph}). Entre algumas de nossas motivações com Web Audio API, estão a síntese, ganho, equalização, expansão multicanal (estereofonia de 2.0 e 5.1 canais) e efeitos (reverb). Ainda, através da Web Audio API podemos tocar um arquivo de áudio (ou \emph{buffers} sintetizados em tempo real por osciladores) em um tempo qualquer, com precisão, além de poder acessar amostras de áudio para análise espectral, por exemplo.

\subsection{Síntese e sampling}

Afim de ressaltar a facilidade do uso de Web Audio para a criação de aplicações Web para processamento de áudio, apresentamos aqui um breve tutorial de seu uso.

Antes de fazer qualquer operação de processamento de áudio, devemos criar um \emph{contexto}. Ali o áudio será processado~\footnote{O conceito de contexto foi inserido em Web Audio para ser compatível com o contexto $<canvas>$ do padrão HTML5}. Criamos um contexto através das seguintes instruções:

\begin{verbatim}
var contexto = new webkitAudioContext();
\end{verbatim}

Não há a necessidade de se criar mais de um contexto de áudio para uma mesma página Web. É importante notar que o prefixo \emph{webkit} usado no construtor da classe \emph{AudioContext} é utilizado provisoriamente, até não se ter um padrão único em uso por todos os navegadores.

Após a criação do contexto, podemos criar um \emph{buffer} de áudio para podermos ler arquivos de áudio a partir de um servidor Web remoto ou de arquivos enviados localmente pelo visitante da página. É uma prática comum carregar os arquivos de áudio logo na inicialização da página, pois assim evitamos atrasos na execução. Todo o \emph{buffer} de áudio é não-persistente, e portanto, podemos refazer sua leitura no momento que desejarmos e quantas vezes forem necessárias, sem termos de reler o arquivo de áudio.

O carregamento de um arquivo de áudio é feito através de uma requisição assíncrona por HTTP. Desta forma, fazemos um pedido ao servidor Web para abrir o arquivo de áudio especificado (no caso do exemplo a seguir, o arquivo de nome \emph{foo.wav}) e informamos uma função (\emph{insereNoBuffer}) que irá ler o arquivo em um \emph{buffer}. A requisição é dita assíncrona pois o processo principal de JavaScript não ficará em um estado de espera até o arquivo ser lido e armazenado em \emph{buffer}.

\begin{verbatim}
requisicao = new XMLHttpRequest();

requisicao.open('GET', 'foo.wav', true);

requisicao.responseType = 'arraybuffer';

requisicao.addEventListener('load', insereNoBuffer, false);

requisicao.send();
\end{verbatim}

A função \emph{insereNoBuffer}, por sua vez, recebe a requisição e cria um \emph{buffer} fazendo uma chamada ao contexto (todos os métodos e construtores para processamento de áudio são acessíveis a partir do contexto). Por fim, uma variável é usada para referenciar o \emph{buffer}.

\begin{verbatim}
var fonte;

function insereNoBuffer(evento) {
    var req = evento.target;

    var bufFonte = contexto.createBufferSource();

    bufFonte.buffer = contexto.createBuffer(req.response, false);

    fonte = bufFonte;
}
\end{verbatim}

O processo de se requisitar, ler e inserir em \emph{buffer} é certamente a operação mais complexa e prolíxa. As demais operações possuem uma interface bastante intuitiva e familiar a engenheiros de áudio, por se basear em grafos de unidades.

Além de carregar um arquivo de áudio, é possível sintetizar ondas comuns através de osciladores.

\begin{verbatim}
var fonte = contexto.createOscillator();

fonte.type = 0;
\end{verbatim}

Neste caso estamos criando uma onda senoidal, pois especificamos seu tipo como $0$. Podemos ainda criar os tipos de onda quadrada, dente de serra e triangular, que possuem os identificadores 1, 2 e 3, respectivamente.

Após termos criado nossa \emph{fonte} de áudio, seja inicializando-o com amostras de um arquivo ou sintetizadas por um oscilador, podemos executá-lo. Para tal, devemos conectá-lo a um \emph{destino} no contexto.

\begin{verbatim}
fonte.connect(contexto.destination);
\end{verbatim}

Para tocarmos o conteúdo da \emph{fonte}, usamos o método \emph{noteOn}, especificando o tempo (em segundos) quando deverá ser tocado. No caso do exemplo a seguir, tocamos a \emph{fonte} no exato momento da chamada do método.

\begin{verbatim}
buffer.noteOn(0);
\end{verbatim}

\subsection{Filtros e mixagem}

Por utilizar um paradigma baseado em grafos, a programação com Web Audio torna-se facilitada e familiar ao engenheiro de áudio. Assim como em um sintetizador modular, onde fios (ou \emph{cord patches}) são usados para ligar um módulo ao outro, podemos adicionar uma infinidade de unidade de áudio entre uma \emph{fonte} e um \emph{destino}, interligando-os da maneira que acharmos interessante para nossos fins.

Um exemplo comum em processamento de áudio é utilizar um nó para controle de volume, um filtro baseado em convolução e um compressor para controle dinâmico do sinal sendo sintetizado. Em Web Audio, criamos os nós fazendo chamadas a métodos com o prefixo \emph{create} e armazemos referências a essas unidades que são em seguidas conectadas umas as outras, e por fim, ao destino.

\begin{verbatim}
var compressor = contexto.createDynamicsCompressor();

var reverb = contexto.createConvolver();

var volume = contexto.createGainNode();

fonte.connect(compressor);

compressor.connect(reverb);

reverb.connect(volume);

volume.connect(contexto.destination);

fonte.noteOn(0);
\end{verbatim}

\section{APLICAÇÕES: USO DE WEB AUDIO PARA PRODUÇÃO MUSICAL}

Nessa seção abordamos aplicações que utilizam Web Audio para a produção musical. Através da Web Audio API um novo horizonte de possibilidades se abriu para o desenvolvimento de aplicações voltadas para áudio. Por ser acessível através de uma interface concisa a partir de JavaScript, pode ser usada em conjunto com outras tecnologias Web (e.g.\ WebGL) para a criação de efeitos sonoros em jogos, aplicações em visualização musical de forma criativa, sistemas para produção musical com alta fidelidade.

Apresentamos aqui sistemas que foram implementados utilizando a Web Audio API para a mixagem de áudio (Mix.JS), uma linguagem de dataflow para manipulação multimídia (Dataflow-WebAudio) e um sistema para composição em tempo real de um estilo de performance em Música Eletroacústica conhecido como \emph{live coding} (Vivace), criado pelos autores, e o qual abordamos em maiores detalhes.

\subsection{MIX.JS}

Desenvolvido por Kevin Ennis\footnote{http://www.kevincennis.com}, apresenta uma simples interface semelhante a uma mesa de som e um playback da obra \emph{1901}, do grupo "Pheonix" ; apresenta uma \emph{pista} com amostras de instrumentos diversos, controles de ganho, panoramização e visualização do sinal de áudio em um \emph{VUmeter}. O usuário pode experenciar a etapa de mixagem dos materiais já criados, gravados e editados. 

\begin{figure}[htpb]
  \begin{center}
    \includegraphics[scale=.4]{mixer.png}
    \caption{O ambiente vivace}
    \label{fig:vivace}
  \end{center}
\end{figure}

\subsection{DATAFLOW-WEBAUDIO}

Desenvolvido por Forrest Oliphant\footnote{http://forresto.com/}, apreenta uma interface de grafos, onde "caixas" são módulos de áudio e o usuário pode criar uma série de combinações de roteamento entre eles. Se assemelha a \emph{softwares} como Max/MSP e PureData.

\begin{figure}[htpb]
  \begin{center}
    \includegraphics[scale=.4]{dataflow.png}
    \caption{Dataflow}
    \label{fig:vivace}
  \end{center}
\end{figure}

\subsection{VIVACE}

Desenvolvido pelo grupo LabMacambira, viabilizamos um espaço virtual compartilhado como meio para edição e mixagem em performances artísticas. Neste ambiente \ref{figure:vivace} pressupomos a manipulação de \emph{vozes}, que são representações de amostras em formato \emph{.wav} e/ou \emph{.mp4}. Com uso das tecnologias \emph{Web Audio API, share.js e dat.GUI}, desenvolvemos um sistema para edição e mixagem colaborativa:

\begin{figure}[htpb]
  \begin{center}
    \includegraphics[scale=.4]{fig_vivace.png}
    \caption{O ambiente vivace}
    \label{figure:vivace}
  \end{center}
\end{figure}

A figura acima apresenta:

\begin{itemize}
    \item Edição de um código-fonte (que chamaremos de vivace-lang), responsável pela manipulação dos materiais sonoros.
    \item Um mixer compactado com um controle básico de estereofonia (ganho, panoramização, equalizador de 3 bandas).
\end{itemize}

\subsubsection{WEB AUDIO API E VIVACE}

Na figura \ref{figure:graph} temos representado uma corrente de nós para cada voz no Vivace. A estratégia utilizada para roteamento, foi criar uma definição de um \emph{mixer} (com as propriedades \emph{gain, pan, high, medium, low}) e aplicá-la através de uma simples sintaxe de grafos:

\begin{verbatim}
# UMA DEFINIÇÃO DE UM MIXER
_mixerDef = function(){
	this.pan = 0
    this.compressor = 0
    ths.reverbTime = 0.1
    this.gain = Math.sqrt(2)
    this.high = 2000
    this.Q_high = 0.5
    this.gain_high = 0
    this.medium = 2000
    this.Q_medium = 0.5
    this.gain_medium = 0
    this.low = 2000
    this.Q_low = 0.5
    this.gain_low = 0

# PROCESSO DE ROTEAMENTO (FEITO PARA CADA VOZ)
# PODEMOS ROTEAR NA FORMA: 
method = "source=>compressor=>reverb=>low=>medium=>high=>gain=>pan=>destination"
router('minhavoz', new _mixerDef(), method)
\end{verbatim}

\begin{figure}[htpb]
  \begin{center}
    \includegraphics[scale=.5]{fig_chain.png}
    \caption{Grafo de unidades de áudio e roteamento entre elas no Vivace}
    \label{figure:graph}
  \end{center}
\end{figure}


\subsubsection{VIVACE-LANG}

Vivace-lang é uma linguagem de domínio específico, parte do ambiente Vivace. A criação de uma nova linguagem proporciona o uso de construções sintáticas flexíveis que permitem produzir musicas de maneira simples e mais familiar a músicos e até mesmo pessoas sem treinamento musical ou computacional. Essa liberdade em se ter uma linguagem que atenda à necessidades técnicas e artísticas permite uma maior flexibilidade de improvisação. E a improvisação, por conseguinte, encontra lugar comum nas práticas de \emph{live coding}. Ter uma linguagem de sintaxe simples, com poucas regras e comandos, diminuem a ocorrência de erros durante a sessão.

Durante a edição do código vivace-lang, realizamos operações comuns na prática de composição musical (reverter, inverter, transpor), aplicadas aos \emph{frames} das amostras de áudio \cite{fabbri2013}: 

No Vivace caracterizamos a essas amostras digitais, propriedades de notas musicais. Canonicamente, as notas possuem ao menos duração, volume, altura e timbre(LACERDA in FABBRI) e são passíveis de serem  tratadas quantitativamente (ROEDERER in FABBRI):

\begin{itemize}
    \item \emph{pos}: posições da amostra (no buffer da voz) a serem tocadas, em segundos.
    \item \emph{dur}: durações da amostra selecionada na voz, em segundos.
    \item \emph{gdur}: durações de cada grão sonoro, da voz, em segundos.
\end{itemize}

Esses valores podem ser \emph{literais}, \emph{operados} ou \emph{gerados}:

\begin{verbatim}
#CRIAÇÃO E EDIÇÃO
foo.pos = [1, 2, 3]
foo.pos = [.1, .2, .3] reverse                         
foo.pos = [1, 2, 3] inverse 
foo.pos = [1, 2, 3] transpose +2  
foo.dur = [2, 3, 4]  
foo.dur = [2, 3, 4] reverse 
foo.dur = [2, 3, 4] inverse
foo.dur = [2, 3, 4] transpose +1

# podemos gerar esses valores
foo.pos = [1/i+1 for i in [1, 2, 3]]

# ou combina-los com as operações
foo.dur = [1/i+1 for i in [1, 2, 3]] reverse

#MIXAGEM
foo.amp = 1                                   
bar.amp = 0.5
baz.amp = 0.25

foo.pan = -1    #Esquerdo
bar.pan = 0    #centro
baz.pan = 1    #direito

foo.high = (freq: 2000, Q: 0.5, amp: 0.75)
bar.medium = (freq: 800, Q: 1, amp: 0.4)
baz.low = (freq: 200, Q: 0.5, amp: 0.67)
\end{verbatim}

\subsubsection{VIVACE e  dat.GUI}

Compreendemos também que este ambiente pode não ser totalmente familiar a músicos ou engnehieos de som; utilizar apenas o vivace-lang tornaria a experiência menos frutífera. Com o uso da biblioteca dat.GUI\footnote{http://workshop.chromeexperiments.com/examples/gui/}, temos uma pequena interface gráfica que lembra uma mesa de som, constituída de \emph{sliders}.

\begin{figure}[htpb]
  \begin{center}
    \includegraphics[scale=.4]{datgui.png}
    \caption{interface gráfica do mixer em Vivace}
    \label{figure:vivace}
  \end{center}
\end{figure}

\section{CONCLUSÕES}

O uso de uma API como Web Audio permite a exploração de aplicações de grande interesse para a engenharia de áudio, composição musical e demais atividades que envolvam o processamento sonoro. Por sua facilidade de programação, possibilita a prototipação de aplicações sonoras, novos instrumentos e experimentações em Música Eletroacústica.

Apresentamos a API através de exemplos de simples compreensão e aplicações que foram desenvolvidas integralmente pelos autores, assim como outras aplicações e bibliotecas em JavaScript que foram desenvolvidas utilizando Web Audio. É possível notar nessas aplicações que, embora tendo apenas alguns poucos anos de desenvolvimento, a existência desta API acabou por criar uma comunidade de músicos e engenheiros de áudio voltados à sua utilização.

Ressaltamos, ao descrever o sistema Vivace, detalhes sobre seu desenvolvimento e como o uso de Web Audio levou à criação de um sistema completo de composição musical em tempo real, voltada a prática de \emph{live coding}. Ainda, a Web por ter natureza descentralizada e colaborativa, permitiu a criação de uma ferramenta como \emph{Mix.JS}, onde pistas de áudio podem ser mixadas por várias pessoas ao mesmo tempo. A facilidade de prototipação, por sua vez, leva a criação de aplicações como \emph{Dataflow}, onde o autor criou uma interface visual para a manipulação de nós de áudio disponíveis na Web Audio, imitando ambientes como \emph{Max/MSP} e \emph{Puredata}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% BIBLIOGRAPHY EXAMPLE
%
% para gerar a bibliografia execute "bibtex template"
% to generate the bibliography please execute "bibtex template"
\bibliographystyle{aes} % style aes.bst
\bibliography{bib} % bibliography file in bibtex format
%
\end{document}